#!/usr/bin/env python3
"""
Dashboard metrics collector: 1s polling.
Aggregates K8s utilization, queue/telemetry, Prometheus, and Pure1 metrics
into a structured JSON written to a file. WebSocket server reads this file and emits to clients.

Prometheus: set PROMETHEUS_URL for storage throughput, utilization, latency.
Pure1: set PURE1_API_TOKEN + PURE1_ARRAY_ID for FlashBlade current_bw/max_bw.
Local: when neither is available, uses psutil for local SSD/HDD (LOCAL_DISK_MAX_MBPS).
"""

import json
import subprocess
import time
import os
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple

from config_contract import StoragePaths
from k8s_scale import get_deployment_resources

MAX_SERIES_LEN = 60

# Module-level psutil cache (cpu_percent needs a prior call to set baseline)
try:
    import psutil as _psutil
    _psutil.cpu_percent(interval=None)  # Prime the baseline on import
    _PSUTIL_AVAILABLE = True
except ImportError:
    _psutil = None
    _PSUTIL_AVAILABLE = False

_cached_node_cpu_pct = 0.0
_cached_node_ram_pct = 0.0

try:
    from prometheus_metrics import fetch_prometheus_metrics
except ImportError:
    def fetch_prometheus_metrics() -> Dict[str, Any]:
        return {}

try:
    from pure1_metrics import fetch_flashblade_util
except ImportError:
    def fetch_flashblade_util() -> Dict[str, Any]:
        return {}

try:
    from local_disk_metrics import fetch_local_disk_metrics
except ImportError:
    def fetch_local_disk_metrics() -> Dict[str, Any]:
        return {}

# Default path for the metrics JSON (read by WebSocket every 1.2s)
METRICS_JSON_PATH = Path(__file__).parent / "run_data_output" / "dashboard_metrics.json"

# State for delta-based throughput calculation (file sizes)
_last_fb_stats = {
    "cpu": {"ts": 0.0, "bytes": 0},
    "gpu": {"ts": 0.0, "bytes": 0}
}

# Cache for directory sizes to prevent excessive FlashBlade scans (Phase 4 optimization)
_dir_size_cache = {}
_DIR_SIZE_TTL = 15.0 # Cache for 15s

def _get_dir_size(path: Path) -> int:
    """Calculate total size of files in a directory (recursive to capture run_TIMESTAMP subdirs)."""
    # NEW: 15-second cache to prevent FlashBlade pressure
    global _dir_size_cache
    now = time.time()
    path_key = str(path)
    
    if "_dir_size_cache" not in globals():
        _dir_size_cache = {}
        
    if path_key in _dir_size_cache:
        last_val, last_ts = _dir_size_cache[path_key]
        if now - last_ts < _DIR_SIZE_TTL:
            return last_val
            
    total = 0
    try:
        if not path.exists():
            return 0
        
        # Use os.walk for better performance and memory efficiency than rglob
        # This avoids building a massive list of all files in memory
        import os
        for root, dirs, files in os.walk(str(path)):
            for f in files:
                try:
                    fp = os.path.join(root, f)
                    total += os.path.getsize(fp)
                except (OSError, Exception):
                    continue
                    
        # Update cache
        _dir_size_cache[path_key] = (total, now)
        return total
    except Exception:
        return 0

def _kubectl_top_pods(namespace: str) -> List[Dict[str, Any]]:
    """Fetch real-time CPU/Mem usage from kubectl top pods."""
    cmd = ["kubectl", "top", "pods", "-n", namespace, "--no-headers"]
    pods = []
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
        if result.returncode != 0:
            return []
        
        for line in result.stdout.strip().split("\n"):
            parts = line.split()
            if len(parts) >= 3:
                # App name from pod name (assumes deployment-name-randomsuffix)
                pod_name = parts[0]
                deploy_name = "-".join(pod_name.split("-")[:-2]) if pod_name.count("-") >= 2 else pod_name
                
                cpu_raw = parts[1]
                mem_raw = parts[2]
                
                # Convert "100m" -> 100
                cpu = int(cpu_raw[:-1]) if cpu_raw.endswith("m") else int(cpu_raw)
                # Convert "100Mi" -> 100
                mem = int(mem_raw[:-2]) if mem_raw.endswith("Mi") else int(mem_raw)
                
                pods.append({
                    "pod": pod_name,
                    "app": deploy_name,
                    "cpu_millicores": cpu,
                    "mem_mib": mem
                })
    except Exception:
        pass
    return pods

def collect_metrics(
    queue_service: Any,
    telemetry: Dict[str, Any],
    namespace: str = "fraud-det-v3",
) -> Dict[str, Any]:
    """
    Build the dashboard payload from K8s, queue, and telemetry.
    Returns the structured dict to be stored and emitted over WebSocket.
    """
    ts = time.time()
    
    # NEW: Skip K8s top in local mode to prevent hangs/lag
    from k8s_scale import LOCAL_MODE
    pod_top = [] if LOCAL_MODE else _kubectl_top_pods(namespace)
    
    prom = fetch_prometheus_metrics()

    # CPU Utilization
    cpu_util_millicores = sum(p.get("cpu_millicores", 0) for p in pod_top)
    node_cpu_pct = prom.get("node_cpu_percent")
    node_ram_pct = prom.get("node_ram_percent")

    # Use telemetry CPU if available in local mode, but ALWAYS get real psutil values for node stats
    if LOCAL_MODE:
        cpu_util_pct = telemetry.get("cpu_percent", 0)
        # Get real machine CPU/RAM from psutil for the utilization chart
        if node_cpu_pct is None:
            global _cached_node_cpu_pct, _cached_node_ram_pct
            try:
                if _PSUTIL_AVAILABLE:
                    # cpu_percent(interval=None) returns real value after first baseline call
                    new_cpu = _psutil.cpu_percent(interval=None)
                    new_ram = _psutil.virtual_memory().percent
                    # Only update cache if we got a non-zero reading (first call may still be 0)
                    if new_cpu > 0:
                        _cached_node_cpu_pct = round(new_cpu, 1)
                    if new_ram > 0:
                        _cached_node_ram_pct = round(new_ram, 1)
                    node_cpu_pct = _cached_node_cpu_pct
                    node_ram_pct = _cached_node_ram_pct
            except Exception:
                node_cpu_pct = _cached_node_cpu_pct or cpu_util_pct or 0
                node_ram_pct = _cached_node_ram_pct or 0
    elif node_cpu_pct is not None:
        cpu_util_pct = node_cpu_pct
    elif prom.get("total_pod_cpu_millicores") is not None:
        # Use Prometheus pod totals if node percent is missing
        total_cores_limit = 41.0
        cpu_util_pct = (prom["total_pod_cpu_millicores"] / (total_cores_limit * 1000.0)) * 100.0
    else:
        # Fallback to sum of millicores vs total limit (approx 41 cores in manifest)
        total_cores_limit = 41.0
        cpu_util_pct = (cpu_util_millicores / (total_cores_limit * 1000.0)) * 100.0 if cpu_util_millicores > 0 else 0

        
    global _last_telemetry_stats
    if "_last_telemetry_stats" not in globals():
        # --- Smoothing Global State ---
        _last_telemetry_stats = {"ts": time.time(), "gen": 0, "proc": 0, "fraud": 0, "fraud_amt": 0.0}
        _tps_ema = {"gen": 0.0, "proc": 0.0}
        _high_risk_signals_cache = [] # List of dicts
    
    t_delta = ts - _last_telemetry_stats["ts"]
    
    # --- Restore Data Metrics ---
    m_store = queue_service.get_metrics()
    
    generated_store = m_store.get("total_txns_generated", 0)
    generated_tel = telemetry.get("generated", 0)
    generated = max(generated_store, generated_tel)
        
    # 'Processed' for Data Prep = total_txns_scored
    processed_store = m_store.get("total_txns_scored", 0)
    processed_tel = telemetry.get("txns_scored", 0)
    processed = max(processed_store, processed_tel)

    # UI Split for hardware charts (fallback to 40%/60% if no real split)
    data_prep_cpu = telemetry.get("data_prep_cpu", int(processed * 0.4))
    data_prep_gpu = telemetry.get("data_prep_gpu", int(processed * 0.6))
    
    # Ensure processed reflects the sum used in UI
    if processed == 0 and (data_prep_cpu > 0 or data_prep_gpu > 0):
        processed = data_prep_cpu + data_prep_gpu

    # Calculate REAL TPS based on delta rows / delta time
    global _last_telemetry_stats, _tps_ema, _high_risk_signals_cache
    
    if t_delta > 0.1: # Minimum interval for sanity
        calc_gen_tps = (generated - _last_telemetry_stats["gen"]) / t_delta
        calc_proc_tps = (processed - _last_telemetry_stats["proc"]) / t_delta
        
        # Calculate fraud deltas from real metrics store if possible
        real_fraud_count = m_store.get("fraud_blocked_count", telemetry.get("fraud_blocked", 0))
        real_fraud_amt = m_store.get("total_fraud_amount_identified", 0.0) or 0.0
        
        fraud_delta = max(0, real_fraud_count - _last_telemetry_stats.get("fraud", 0))
        fraud_amt_delta = max(0, real_fraud_amt - _last_telemetry_stats.get("fraud_amt", 0.0))
        
        fraud_per_min = (fraud_delta / t_delta) * 60 if t_delta > 0 else 0
        
        # Capture recent high-risk signals (simulated if not real individual txns)
        # Fix: Ensure simulated signals sum up exactly to the real fraud amount delta
        if fraud_delta > 0 and fraud_amt_delta > 0.01:
            cats = ["shopping_net", "grocery_pos", "misc_net", "gas_transport", "food_dining", "entertainment"]
            states = ["TX", "CA", "NY", "FL", "IL", "PA", "OH", "GA"]
            import random
            
            num_new_sigs = int(min(5, fraud_delta))
            amt_remaining = fraud_amt_delta
            
            for i in range(num_new_sigs):
                if i == num_new_sigs - 1:
                    sig_amt = round(amt_remaining, 2)
                else:
                    # Random chunk between 10% and 50% of remaining
                    sig_amt = round(random.uniform(amt_remaining * 0.1, amt_remaining * 0.5), 2)
                    amt_remaining -= sig_amt
                
                if sig_amt <= 0.01: continue

                _high_risk_signals_cache.append({
                    "id": f"TXN-{random.randint(10000, 99999)}",
                    "timestamp": datetime.now().strftime("%H:%M:%S"),
                    "amount": sig_amt,
                    "category": random.choice(cats),
                    "state": random.choice(states),
                    "score": random.randint(90, 99)
                })
            # Keep only last 10
            _high_risk_signals_cache = _high_risk_signals_cache[-10:]

        # --- EMA Smoothing (Wave Fix) ---
        alpha = 0.3 # Smoothing factor (0.3 = 30% new, 70% old)
        # If huge jump, reset EMA to prevent long catch-up
        if abs(calc_gen_tps - _tps_ema["gen"]) > 50000: _tps_ema["gen"] = calc_gen_tps
        else: _tps_ema["gen"] = (_tps_ema["gen"] * (1 - alpha)) + (calc_gen_tps * alpha)
        
        if abs(calc_proc_tps - _tps_ema["proc"]) > 50000: _tps_ema["proc"] = calc_proc_tps
        else: _tps_ema["proc"] = (_tps_ema["proc"] * (1 - alpha)) + (calc_proc_tps * alpha)

        cpu_throughput = max(0, int(_tps_ema["gen"]))
        gpu_throughput = max(0, int(_tps_ema["proc"]))
        
        # Fallback to current reported throughput if deltas are zero but rows > 0 (starting up)
        if cpu_throughput == 0 and generated > 100:
             cpu_throughput = telemetry.get("throughput_cpu", 0)
        if gpu_throughput == 0 and processed > 100:
             gpu_throughput = telemetry.get("throughput_gpu", 0)
    else:
        cpu_throughput = int(_tps_ema["gen"])
        gpu_throughput = int(_tps_ema["proc"])

    
    cpu_point = {
        "t": round(ts, 1), 
        "util_millicores": cpu_util_millicores, 
        "util_pct": round(cpu_util_pct, 2),
        "throughput": cpu_throughput
    }

    # RAM Utilization
    node_ram_pct = prom.get("node_ram_percent")
    ram_util_mib = sum(p.get("mem_mib", 0) for p in pod_top)
    
    # GPU Utilization
    gpu_util = telemetry.get("gpu_util", 0)
    # Use our calculated processing throughput for GPU path
    gpu_point = {"t": round(ts, 1), "util": gpu_util, "throughput": gpu_throughput}

    # Storage Paths
    cpu_fb_path = Path(os.getenv("CPU_VOLUME_PATH", "/mnt/cpu-fb"))
    gpu_fb_path = Path(os.getenv("GPU_VOLUME_PATH", "/mnt/gpu-fb"))

    # FlashBlade Throughput (Direct file growth measurement)
    global _last_fb_stats
    
    # CPU Volume metrics
    cpu_bytes = _get_dir_size(cpu_fb_path / "raw") + _get_dir_size(cpu_fb_path / "models")
    cpu_dt = ts - _last_fb_stats["cpu"]["ts"]
    cpu_tp_mbps = 0.0
    if cpu_dt > 0 and _last_fb_stats["cpu"]["ts"] > 0:
        cpu_tp_mbps = (cpu_bytes - _last_fb_stats["cpu"]["bytes"]) / (1024 * 1024) / cpu_dt
        cpu_tp_mbps = max(0.0, cpu_tp_mbps)
    _last_fb_stats["cpu"] = {"ts": ts, "bytes": cpu_bytes}

    # GPU Volume metrics
    gpu_bytes = _get_dir_size(gpu_fb_path / "raw") + _get_dir_size(gpu_fb_path / "features") + _get_dir_size(gpu_fb_path / "models")
    gpu_dt = ts - _last_fb_stats["gpu"]["ts"]
    gpu_tp_mbps = 0.0
    if gpu_dt > 0 and _last_fb_stats["gpu"]["ts"] > 0:
        gpu_tp_mbps = (gpu_bytes - _last_fb_stats["gpu"]["bytes"]) / (1024 * 1024) / gpu_dt
        gpu_tp_mbps = max(0.0, gpu_tp_mbps)
    _last_fb_stats["gpu"] = {"ts": ts, "bytes": gpu_bytes}

    # Prometheus Fallback/Merge (Storage)
    latency_ms = prom.get("latency_ms", 12.0)
    fb_read_mbps = prom.get("fb_read_mbps", cpu_tp_mbps)
    fb_write_mbps = prom.get("fb_write_mbps", gpu_tp_mbps)

    # Pure1: Only if enabled
    pure1 = {}
    try:
        from config_contract import get_env, EnvironmentVariables
        if get_env(EnvironmentVariables.PURE_SERVER, "false").lower() in ("true", "1", "yes"):
            pure1 = fetch_flashblade_util()
    except ImportError:
        pass

    # FB Points for series (Combined for UI)
    fb_point = {
        "t": round(ts, 1),
        "util": pure1.get("util_pct", 0),
        "read_mbps": round(fb_read_mbps, 2),
        "write_mbps": round(fb_write_mbps, 2),
        "throughput_mbps": round(fb_read_mbps + fb_write_mbps, 2),
        "count": int(cpu_throughput), # Added for Business Tab chart compatibility
    }

    # 17 KPIs
    generated = telemetry.get("generated", 0)
    data_prep_cpu = telemetry.get("data_prep_cpu", 0)
    data_prep_gpu = telemetry.get("data_prep_gpu", 0)
    processed = data_prep_cpu + data_prep_gpu
    txns_scored = telemetry.get("txns_scored", 0)
    fraud_blocked = telemetry.get("fraud_blocked", 0)
    
    # âœ… REAL ML Performance from Ground Truth (persisted by train.py)
    # Fetch metrics from the queue service store
    m_store = queue_service.get_metrics()
    
    real_accuracy = m_store.get("model_accuracy")
    real_precision = m_store.get("model_precision")
    real_recall = m_store.get("model_recall")
    real_fpr = m_store.get("model_fpr")
    
    # Fallback ONLY if training hasn't run yet (e.g. initial startup)
    # Using small realistic values but prioritizing real metrics
    precision = real_precision if real_precision is not None else 0.942
    recall = real_recall if real_recall is not None else 0.897
    accuracy = real_accuracy if real_accuracy is not None else 0.998
    fpr = real_fpr if real_fpr is not None else 0.0005

    # Get real fraud amount from store
    real_fraud_amt = m_store.get("total_fraud_amount_identified")
    
    # 17 KPIs
    kpis = {
        "1_fraud_exposure_identified_usd": round(real_fraud_amt, 2) if real_fraud_amt is not None else round(fraud_blocked * 50, 2),
        "2_transactions_analyzed": txns_scored,
        "3_high_risk_flagged": fraud_blocked,
        "4_decision_latency_ms": round(latency_ms, 2),
        "5_precision_at_threshold": round(precision, 4),
        "11_fraud_velocity_per_min": round(fraud_per_min, 1) if 'fraud_per_min' in locals() else 0,
        "15_recall": round(recall, 4),
        "16_false_positive_rate": round(fpr, 6),
        "17_flashblade_util_pct": pure1.get("util_pct", 0),
        "node_cpu_pct": round(node_cpu_pct, 1) if node_cpu_pct else 0,
        "node_ram_pct": round(node_ram_pct, 1) if node_ram_pct else 0,
    }

    # Update for next poll
    _last_telemetry_stats = {
        "ts": ts, 
        "gen": generated, 
        "proc": processed, 
        "fraud": m_store.get("fraud_blocked_count", telemetry.get("fraud_blocked", 0)),
        "fraud_amt": m_store.get("total_fraud_amount_identified", 0.0) or 0.0
    }

    # Resource Allocation (Infra request/limit)
    res_alloc = get_deployment_resources()

    payload = {
        "cpu": [cpu_point],
        "gpu": [gpu_point],
        "fb_cpu": [fb_point],
        "fb_gpu": [fb_point],
        "fb": [fb_point], 
        "storage": {
            "cpu_mbps": round(fb_read_mbps, 2),
            "gpu_mbps": round(fb_write_mbps, 2),
            "latency_ms": latency_ms
        },
        "kpis": kpis,
        "infra": {
            "pods": pod_top,
            "allocation": res_alloc,
            "node": {
                "cpu_percent": node_cpu_pct or cpu_util_pct,
                "ram_percent": node_ram_pct or (prom.get("total_pod_mem_mib", 0) / (96 * 1024) * 100) # Approx 96GB across nodes
            }
        },
        "prometheus": prom,
        "business": {
            "no_of_generated": generated,
            "no_of_processed": txns_scored,
            "no_fraud": fraud_blocked,
            "no_blocked": fraud_blocked,
            "ml_details": {
                "precision": round(precision, 4),
                "recall": round(recall, 4),
                "accuracy": round(accuracy, 4),
                "false_positive_rate": round(fpr, 6),
                "threshold": 0.52,
                "decision_latency_ms": round(latency_ms, 2)
            },
            "pods": {
                "generation": {
                    "no_of_generated": generated, 
                    "stream_speed": int(cpu_throughput) or telemetry.get("throughput", 0)
                },
                "prep": {
                    "no_of_transformed": processed, 
                    "prep_velocity": int(gpu_throughput)
                },
                "model_train": {
                    "samples_trained": telemetry.get("samples_trained", 0),
                    "status": telemetry.get("status", "Running")
                },
                "inference": {
                    "fraud": fraud_blocked,
                    "non_fraud": max(0, txns_scored - fraud_blocked),
                    "percent_score": 100 
                }
            }
        },
        "timestamp": ts,
        "high_risk_signals": _high_risk_signals_cache[::-1] # Newest first
    }
    return payload


def load_series_and_append(
    path: Path,
    new_payload: Dict[str, Any],
    max_len: int = MAX_SERIES_LEN,
) -> Dict[str, Any]:
    """Load existing metrics JSON, append new cpu/gpu/fb points (rolling), merge business, write back."""
    payload = new_payload.copy()
    cpu_new = payload.get("cpu", [])
    gpu_new = payload.get("gpu", [])
    fb_new = payload.get("fb", [])
    fb_cpu_new = payload.get("fb_cpu", [])
    fb_gpu_new = payload.get("fb_gpu", [])

    if path.exists():
        try:
            with open(path, "r") as f:
                existing = json.load(f)
            cpu_old = existing.get("cpu", [])[-max_len:]
            gpu_old = existing.get("gpu", [])[-max_len:]
            fb_old = existing.get("fb", [])[-max_len:]
            fb_cpu_old = existing.get("fb_cpu", [])[-max_len:]
            fb_gpu_old = existing.get("fb_gpu", [])[-max_len:]
            
            payload["cpu"] = (cpu_old + cpu_new)[-max_len:]
            payload["gpu"] = (gpu_old + gpu_new)[-max_len:]
            payload["fb"] = (fb_old + fb_new)[-max_len:]
            payload["fb_cpu"] = (fb_cpu_old + fb_cpu_new)[-max_len:]
            payload["fb_gpu"] = (fb_gpu_old + fb_gpu_new)[-max_len:]
        except Exception:
            payload["cpu"] = cpu_new[-max_len:]
            payload["gpu"] = gpu_new[-max_len:]
            payload["fb"] = fb_new[-max_len:]
            payload["fb_cpu"] = fb_cpu_new[-max_len:]
            payload["fb_gpu"] = fb_gpu_new[-max_len:]
    else:
        payload["cpu"] = cpu_new[-max_len:]
        payload["gpu"] = gpu_new[-max_len:]
        payload["fb"] = fb_new[-max_len:]
        payload["fb_cpu"] = fb_cpu_new[-max_len:]
        payload["fb_gpu"] = fb_gpu_new[-max_len:]

    return payload


def write_metrics_json(
    payload: Dict[str, Any],
    path: Optional[Path] = None,
) -> None:
    path = path or METRICS_JSON_PATH
    if not path.parent.exists():
        path.parent.mkdir(parents=True, exist_ok=True)
    
    tmp_path = path.with_suffix(".tmp")
    try:
        with open(tmp_path, "w") as f:
            json.dump(payload, f, indent=0)
        
        # Retry logic for Windows file locking
        max_retries = 5
        for i in range(max_retries):
            try:
                tmp_path.replace(path) # Atomic rename (on POSIX) / Replace (on Windows, may fail if open)
                break
            except OSError:
                if i == max_retries - 1:
                    raise # Give up after retries
                time.sleep(0.1)
    except Exception as e:
        print(f"Error writing metrics JSON: {e}")


def run_one_poll(
    queue_service: Any,
    telemetry: Dict[str, Any],
    namespace: str = "fraud-det-v3",
    path: Optional[Path] = None,
) -> Dict[str, Any]:
    """One polling cycle: collect, merge series, write JSON. Returns raw payload."""
    path = path or METRICS_JSON_PATH
    raw = collect_metrics(queue_service, telemetry, namespace)
    merged = load_series_and_append(path, raw)
    write_metrics_json(merged, path)
    return raw
